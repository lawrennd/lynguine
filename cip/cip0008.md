---
id: "cip0008"
title: "Lynguine Server Mode for Fast Repeated Access"
status: "Proposed"
priority: "High"
effort: "Large"
type: "architecture"
created: "2026-01-03"
last_updated: "2026-01-03"
owner: "lawrennd"
github_issue: null
dependencies: null
---

# CIP-0008: Lynguine Server Mode for Fast Repeated Access

## Status

- [x] Proposed: 2026-01-03
- [ ] Accepted
- [ ] Implemented
- [ ] Closed

## Description

Implement a server mode for lynguine that allows long-running processes to serve multiple data access requests without repeatedly paying startup costs. This addresses the performance requirement (REQ-0007) where applications like lamd need to access lynguine functionality multiple times in quick succession.

## Motivation

### The Problem

Applications like lamd currently call lynguine multiple times per operation:
- Each call requires full Python interpreter startup
- Import overhead (pandas, numpy, etc.) on every call
- Configuration parsing and initialization on every call
- Total time = (startup_cost Ã— N_calls) + actual_work

**Current Performance** (needs measurement):
- Startup time: ? seconds (to be profiled)
- Per-operation overhead: ? seconds
- Total for N operations: startup Ã— N

This makes lynguine impractical for:
- CLI tools that process batches of files
- Applications with frequent data access patterns
- Interactive workflows requiring multiple queries

### Why This Matters

**For lamd**:
- Needs to access multiple lynguine-managed files per operation
- Startup overhead dominates execution time
- Poor user experience

**For future applications**:
- Limits adoption for CLI tools
- Makes batch processing impractical
- Prevents interactive use cases

## Investigation and Analysis

### Profiling Results (Completed 2026-01-03)

**Environment**: macOS, Python 3.11 (conda), lynguine in development mode

**Key Finding**: Import costs dominate startup time at **~1.9 seconds per cold start**.

#### Startup Time Breakdown

```
Major Dependency Import Times:
  pandas              :  1.223s  (63% of total)
  numpy               :  0.000s  (already imported by pandas)
  yaml                :  0.019s  (1%)
  liquid              :  0.154s  (8%)

Lynguine Component Import Times:
  lynguine            :  0.548s  (28% of total)
  (submodules cached after initial import)

Summary:
  Total startup time:   1.947s
  Dependencies:         1.396s (72%)
  Lynguine components:  0.552s (28%)
```

**Analysis**:
- **pandas dominates**: 1.223s (63%) - cannot be optimized, must be amortized
- **lynguine initialization**: 0.548s (28%) - includes pandas triggering
- **Subsequent imports fast**: Cached after first import (Python module cache)

#### Memory Usage

```
Python baseline:           15.2 MB
After lynguine import:    132.4 MB  (+117 MB, mostly pandas/numpy)
After Interface import:   132.4 MB  (no additional overhead)
```

**Analysis**: Memory overhead dominated by pandas/numpy. Total footprint (~132 MB) is reasonable for long-running server process.

#### Performance Impact

**Current (without server mode)**:
- 1 call:    ~2.0s  (1.9s startup + 0.1s work)
- 10 calls:  ~20s   (19s startup + 1s work) - **95% overhead**
- 100 calls: ~200s  (190s startup + 10s work) - **95% overhead**

**Projected (with server mode)**:
- 1 call:    ~2.0s  (1x - no benefit)
- 10 calls:  ~3.0s  (**6-7x faster**)
- 100 calls: ~12s   (**15-20x faster**)

### Conclusion: Server Mode is Justified

**Evidence**:
1. âœ… Startup cost is high (1.9s per call)
2. âœ… Cannot be significantly optimized (pandas is core dependency)
3. âœ… Server mode would provide major benefit (10-20x for repeated calls)
4. âœ… Memory overhead acceptable (132 MB)
5. âœ… Real use case (lamd makes multiple calls per operation)

**Decision**: Proceed with CIP-0008 implementation.

## Applicability: When Server Mode Helps (and When It Doesn't)

### âœ… Applications That Benefit from Server Mode

**Pattern**: **Repeated subprocess calls** from shell/scripts/make

**Characteristics**:
- Multiple independent process launches
- Each call pays full Python + pandas + lynguine startup cost
- Short-lived processes (start â†’ execute â†’ exit)
- Stateless operations

**Examples**:
1. **lamd (primary use case)**:
   - Builds CVs using make with 38+ subprocess calls per build
   - Each `mdfield` call: new process â†’ import lynguine â†’ extract field â†’ exit
   - Each `mdlist` call: new process â†’ import lynguine â†’ process data â†’ exit
   - **Current overhead**: ~72 seconds startup per CV build
   - **With server mode**: ~0.34 seconds (**210x faster**)

2. **CLI tools and scripts**:
   - Batch processing scripts calling lynguine repeatedly
   - Shell pipelines with multiple lynguine operations
   - Automated workflows with repeated lynguine access

3. **CI/CD pipelines**:
   - Multiple validation steps each importing lynguine
   - Repeated configuration checks
   - Data processing stages

**Server mode provides**: 10-200x speedup depending on number of operations

### âŒ Applications That DON'T Benefit from Server Mode

**Pattern**: **Long-running Python processes** (already amortize startup)

**Characteristics**:
- Single long-lived Python process
- Imports done once at process start
- Pandas/lynguine stay loaded in memory
- Stateful operations with persistent data

**Examples**:
1. **referia (Jupyter notebooks)**:
   - Jupyter kernel runs continuously
   - Startup cost paid once when kernel starts
   - pandas/lynguine/referia already imported and cached
   - Interactive cell-by-cell execution
   - **Server mode provides**: No benefit (startup already amortized)
   - **If referia is slow**: Different causes (I/O, data processing, rendering)
   - **Solutions**: Caching, lazy evaluation, optimize pandas operations

2. **Long-running web applications**:
   - Django/Flask apps import lynguine at startup
   - Process pool keeps modules loaded
   - Application server handles persistence

3. **Jupyter notebooks (general)**:
   - Kernel stays loaded during session
   - Imports persist across cells

**Server mode provides**: No benefit (may add unnecessary IPC overhead)

### ðŸ¤” Hybrid Scenarios

**Pattern**: Short Python scripts calling lynguine multiple times internally

**Example**:
```python
# script.py - called once from shell
from lynguine.config.interface import Interface

# Multiple operations within same script
for config_file in files:
    interface = Interface.from_file(config_file)
    # process...
```

**Analysis**:
- Single subprocess call (startup paid once)
- Multiple lynguine operations internally (already fast)
- **Server mode provides**: No benefit (startup already amortized within script)

### Summary Table

| Application Pattern | Subprocess Calls | Startup Cost | Server Mode Benefit |
|-------------------|-----------------|-------------|-------------------|
| lamd (make/shell) | Many (38+/build) | ~72s/build | âœ… **210x faster** |
| CLI batch scripts | Many | High | âœ… **10-200x faster** |
| CI/CD pipelines | Multiple steps | High | âœ… **Significant** |
| referia (Jupyter) | One (kernel) | Amortized | âŒ **None** |
| Web apps | One (startup) | Amortized | âŒ **None** |
| Long Python scripts | One | Amortized | âŒ **None** |

### Design Implication

Server mode is an **opt-in optimization** for specific use patterns (repeated subprocess calls), not a general replacement for direct lynguine usage. Applications choose the appropriate mode:

- **Direct import**: Long-running processes (referia, web apps)
- **Server mode**: Repeated subprocess calls (lamd, CLI tools)

## Proposed Solution: Lynguine Server Mode

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Client Applications                       â”‚
â”‚              (lamd, CLI tools, scripts)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼ (lightweight IPC)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Lynguine Server Process                      â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Server Manager                                     â”‚    â”‚
â”‚  â”‚  - Process lifecycle                                â”‚    â”‚
â”‚  â”‚  - Request routing                                  â”‚    â”‚
â”‚  â”‚  - Session management                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â”‚                                    â”‚
â”‚                         â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Lynguine Core (already imported, initialized)     â”‚    â”‚
â”‚  â”‚  - Interface, Context                               â”‚    â”‚
â”‚  â”‚  - CustomDataFrame                                  â”‚    â”‚
â”‚  â”‚  - Compute engine                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Design Decisions

#### 1. Server Lifecycle

**Question**: How is the server started and stopped?

**Proposal**:
- **Auto-start**: Client checks if server running, starts if needed
- **Graceful shutdown**: Timeout after idle period (configurable)
- **Manual control**: Commands to start/stop explicitly

```python
# Client usage (transparent to user)
from lynguine.client import Interface

# Auto-starts server if needed
interface = Interface.from_file("config.yml")
data = interface.read()
# Uses server for all operations
```

#### 2. Communication Protocol

**Critical Consideration**: Windows portability

**Options Evaluated**:

| Protocol | Overhead | Platforms | Complexity | Verdict |
|----------|----------|-----------|------------|---------|
| Unix sockets | ~0.01ms | Unix/Mac only | Low | âŒ Not portable |
| Named Pipes | ~0.01ms | Windows only | Low | âŒ Not portable |
| HTTP/REST | ~1-5ms | All platforms | Medium | âœ… **Recommended** |
| gRPC | ~0.1-1ms | All platforms | High | âš ï¸ Overkill |

**Key Insight**: Even with HTTP overhead, we win big:
- Startup cost avoided: **1,900ms**
- HTTP overhead: **~2ms per request**
- We can afford **100x more overhead** and still get 10-20x speedup

**Recommendation**: Use **HTTP/REST from the start** (not as Phase 2)

**Rationale**:
- âœ… **Cross-platform**: Works on Unix, macOS, Windows out of the box
- âœ… **Overhead acceptable**: 2ms vs 1,900ms savings = 950x benefit
- âœ… **Simple**: Python's http.server is built-in, well-understood
- âœ… **Debuggable**: Can use curl, browser, standard tools
- âœ… **Remote access**: Client and server can be on different machines
- âœ… **Network scenarios**: Enables distributed computing, shared servers
- âš ï¸ **Only downside**: ~2ms vs ~0.01ms (but irrelevant given 1,900ms savings)
- âš ï¸ **Security**: Must address authentication for remote access (see below)

**Performance Analysis**:
```
Scenario: 100 operations

Without server:
  100 Ã— 1,900ms = 190,000ms = 190 seconds

With local HTTP server (127.0.0.1):
  Startup: 1,900ms
  Requests: 100 Ã— 2ms = 200ms
  Total: 2,100ms = 2.1 seconds
  Improvement: 90x faster

With remote HTTP server (LAN):
  Startup: 1,900ms
  Requests: 100 Ã— 5ms = 500ms  (network latency)
  Total: 2,400ms = 2.4 seconds
  Improvement: 79x faster
  Still excellent!

With remote HTTP server (WAN):
  Startup: 1,900ms
  Requests: 100 Ã— 50ms = 5,000ms  (internet latency)
  Total: 6,900ms = 6.9 seconds
  Improvement: 27x faster
  Still significant, but latency matters

With Unix sockets (local only):
  Startup: 1,900ms
  Requests: 100 Ã— 0.01ms = 1ms
  Total: 1,901ms = 1.9 seconds
  Improvement: 100x faster

Verdict: 
  - Local: HTTP vs Unix sockets = 2.1s vs 1.9s (0.2s difference)
  - Remote: Enables distributed computing scenarios
  - Network latency is acceptable for most use cases
  - Choose HTTP for portability + remote capability
```

#### 3. Data Isolation

**Critical**: Each request must be isolated (no state leakage)

**Implementation**:
- Server maintains no request-level state
- Each operation is stateless
- Configuration passed with each request (or cached with session ID)
- Results serialized and sent back (no shared memory)

#### 4. Error Handling

**Challenges**:
- Server crash should not affect client
- Bad request should not crash server
- Clear error propagation to client

**Implementation**:
- Try/except around all request handlers
- Structured error responses
- Client detects server crash and restarts
- Logging on both client and server sides

#### 5. Security and Remote Access

**HTTP enables remote access** (client and server on different machines)

**Use Cases**:
- **Local-only** (default): Server binds to `127.0.0.1:8765`
  - Fast (~2ms overhead)
  - Secure (not accessible from network)
  - Simplest deployment
  
- **Shared server**: Server binds to `0.0.0.0:8765`
  - Multiple users share one lynguine server
  - Reduces memory (one Python process instead of N)
  - Enables compute clusters
  - **Requires authentication**

**Security Model**:

**Phase 1-3** (Local only, no auth):
- Server binds to `127.0.0.1` only
- Not accessible from network
- Safe for single-user, local scenarios
- Configuration: `LYNGUINE_SERVER_HOST=127.0.0.1` (default)

**Phase 4** (Optional remote access with auth):
- Server can bind to `0.0.0.0` if explicitly configured
- **Requires authentication** (API keys, tokens)
- **Requires HTTPS** (TLS/SSL) for encrypted transport
- Configuration: `LYNGUINE_SERVER_AUTH=required`
- Use case: Shared compute server, team environments

**Authentication Options** (Phase 4):
1. **API Keys**: Simple, token-based authentication
2. **OAuth**: Integrate with organizational auth
3. **SSH Tunneling**: Leverage existing SSH infrastructure

**Decision**: Start with **local-only** (Phase 1-3), add **authenticated remote access** as Phase 4

**Rationale**:
- Most use cases are local (lamd, CLI tools)
- Remote access is valuable but not MVP
- Security is critical for remote access
- Can be added later without breaking changes

### API Design

#### Client API (Backward Compatible)

```python
# Option 1: Transparent local (preferred for most users)
from lynguine import Interface
# Automatically uses local server if available, falls back to direct

# Option 2: Explicit local server
from lynguine.client import ServerInterface
interface = ServerInterface.from_file("config.yml")

# Option 3: Remote server (Phase 4)
from lynguine.client import ServerInterface
interface = ServerInterface.from_file(
    "config.yml",
    server_url="http://compute-server.example.com:8765",
    api_key="your-api-key"  # Required for remote
)

# Option 4: Context manager
from lynguine.server import LynguineServer
with LynguineServer() as server:
    # Multiple operations
    data1 = server.read("file1.yml")
    data2 = server.read("file2.yml")
```

#### Server Commands

```bash
# Start server (local-only, default)
lynguine serve --daemon
# Binds to 127.0.0.1:8765

# Start server on specific port
lynguine serve --port=9000 --daemon

# Stop server
lynguine serve --stop

# Status
lynguine serve --status

# Configure idle timeout
lynguine serve --idle-timeout=300  # 5 minutes

# Phase 4: Remote access (requires authentication)
lynguine serve --host=0.0.0.0 --auth-required --daemon
# WARNING: Only use with authentication enabled
```

### Wire Protocol

**Simple REST API over HTTP**:

```python
# Endpoint structure
POST http://localhost:8765/api/read_data
POST http://localhost:8765/api/write_data
POST http://localhost:8765/api/compute
GET  http://localhost:8765/api/health

# Request format (JSON body)
{
    "config": {...},        # Interface configuration
    "params": {             # Operation-specific parameters
        "path": "...",
        "options": {...}
    }
}

# Response format (JSON)
{
    "status": "success",    # or "error"
    "result": {...},        # serialized data
    "error": null,          # or error message
    "timing": {             # performance metrics
        "server_ms": 10.5,
        "operation_ms": 8.2
    }
}
```

**Data serialization**:
- JSON for metadata and simple data
- Base64-encoded pickle for DataFrames (Python-native, efficient)
- HTTP chunked transfer for large responses
- Alternative: Arrow IPC format (future, for language-agnostic clients)

## Implementation Plan

### Phase 1: Proof of Concept (2 weeks)

**Goal**: Validate approach with minimal, cross-platform implementation

- [x] Profile current startup costs (baseline measurements) - **COMPLETED 2026-01-03**
  - Startup: 1.947s (pandas 1.223s, lynguine 0.548s)
  - Memory: 132 MB
  - Target improvement validated: 10-20x possible
- [ ] Implement basic HTTP server (Python http.server, single-threaded)
- [ ] Implement basic client (transparent connection via requests library)
- [ ] Support one operation: `read_data()`
- [ ] Test on Unix/macOS/Windows
- [ ] Benchmark performance vs direct calls
- [ ] Measure actual HTTP overhead
- [ ] Decision point: Is improvement sufficient?

**Success criteria**:
- Server starts and handles requests on all platforms
- Client can make repeated calls
- Performance improvement > 5x for 10 operations (target: 6-7x based on profiling)
- HTTP overhead < 5ms per request

### Phase 2: Core Features (3-4 weeks)

**Goal**: Production-ready single-user server

- [ ] Support all common operations (read, write, compute)
- [ ] Graceful error handling
- [ ] Auto-start/stop lifecycle
- [ ] Idle timeout
- [ ] Logging and diagnostics
- [ ] Basic tests (unit and integration)
- [ ] Documentation (basic usage)

**Success criteria**:
- lamd can use server mode
- All operations work correctly
- No data corruption or state leakage
- Performance improvement validated

### Phase 3: Robustness (2-3 weeks)

**Goal**: Reliable, production-quality

- [ ] Comprehensive error handling
- [ ] Server crash recovery
- [ ] Request timeout handling
- [ ] Memory leak prevention
- [ ] Comprehensive test suite
- [ ] Performance benchmarks
- [ ] User documentation

**Success criteria**:
- 100+ tests passing
- No memory leaks in long runs
- Clear error messages
- Migration guide for users

### Phase 4: Remote Access and Advanced Features (future)

**Goal**: Enable secure remote access and multi-user scenarios

**Remote Access Features**:
- [ ] Authentication system (API keys, tokens)
- [ ] Authorization (per-user permissions)
- [ ] HTTPS/TLS support (encrypted transport)
- [ ] Server binds to 0.0.0.0 (when explicitly configured)
- [ ] Multi-user support (resource isolation)
- [ ] Rate limiting (per-user quotas)

**Performance Features**:
- [ ] Request caching/memoization
- [ ] Parallel request handling (thread pool)
- [ ] Connection pooling
- [ ] Response compression

**Operations Features**:
- [ ] Monitoring and metrics dashboard
- [ ] Health checks and status API
- [ ] Graceful shutdown with request draining
- [ ] Server clustering (load balancing)

**Use Cases Enabled**:
- Shared compute server for a team
- Centralized lynguine service (reduces memory Ã— N users)
- Cloud-based data processing
- Distributed computing workflows

### Phase 5: Stateful Data Sessions

**Goal**: Efficient interactive data exploration with minimal data transfer

**Problem**: Current server mode transfers entire DataFrames over HTTP for each operation, which is inefficient for large datasets and interactive workflows.

**Proposed Solution**: Stateful sessions where server loads and maintains data in memory, allowing clients to interact via lightweight index/value operations.

**Architecture**:
- Server initializes sessions with lynguine interface files
- Server loads and maintains data in memory (session state)
- Clients send lightweight commands: `set_index()`, `get_value()`, `get_slice()`, etc.
- Only indices and values transfer over HTTP, not full DataFrames
thi

**Client API**:
```python
# Create session (loads data once)
session = client.create_session(interface_file='large_data.yml')

# Lightweight operations (minimal data transfer)
value = session.get_value(index=100, column='name')  # Transfer ~bytes
slice_df = session.get_slice(start=100, end=200)     # Transfer only slice
filtered = session.filter(column='age', operator='>', value=30)

# Cleanup
session.delete()
```

**Features**:
- [ ] Session management (create, list, delete)
- [ ] Session isolation (multiple clients, separate sessions)
- [ ] Session timeout and automatic cleanup
- [ ] Memory limits and monitoring
- [ ] Data operations: get_value, get_slice, get_column, filter, sort, stats
- [ ] Session metadata: shape, columns, memory usage, created time

**Benefits**:
- Dramatically reduced HTTP traffic (100-1000x for interactive workflows)
- Faster interactive exploration
- Server maintains data in memory (already loaded)
- Natural fit for iterative/exploratory workflows

**Use Cases Enabled**:
- Interactive data exploration (Jupyter-like workflows)
- Data quality checks without full DataFrame transfer
- Iterative processing with minimal overhead
- Remote data exploration with bandwidth constraints

## Performance Targets

**Based on profiling** (measured 2026-01-03):

| Scenario | Current | Target | Improvement |
|----------|---------|--------|-------------|
| Single call | ~2.0s | ~2.0s | 1x (no change) |
| 10 calls | ~20s | ~3.0s | **6-7x faster** |
| 100 calls | ~200s | ~12s | **15-20x faster** |

**Actual measurements**:
- âœ… Startup time: 1.947s (measured)
- âœ… Breakdown: pandas 1.223s, lynguine 0.548s, other 0.176s
- âœ… Memory usage: 132 MB (measured)
- â³ Server overhead per request: ~0.01s (estimated, to be validated in PoC)

## Backward Compatibility

**Critical**: Must not break existing code

**Strategy**:
- Server mode is **optional** (not required)
- Direct mode continues to work
- Client API is transparent (auto-detects server)
- Users can opt-in with `use_server=True` or environment variable

**Migration path**:
```python
# Old code (continues to work)
from lynguine import Interface
interface = Interface.from_file("config.yml")

# New code (same API, uses server if available)
from lynguine import Interface
interface = Interface.from_file("config.yml", use_server=True)

# Or via environment variable
export LYNGUINE_USE_SERVER=1
```

## Alternative Approaches Considered

### Alternative 1: Import Optimization Only

**Approach**: Defer heavy imports, optimize existing code

**Pros**:
- No architectural changes
- Simpler implementation
- No process management

**Cons**:
- Limited improvement (maybe 2-3x)
- Doesn't help with configuration parsing
- Still pays cost on every call

**Decision**: Worth doing as quick win, but insufficient alone

### Alternative 2: Persistent REPL

**Approach**: Keep Python process running, execute commands in context

**Pros**:
- Simpler than full server
- No serialization overhead

**Cons**:
- State management complexity
- Hard to isolate requests
- Error recovery difficult

**Decision**: Rejected - too fragile

### Alternative 3: In-Memory Caching

**Approach**: Cache loaded data in memory across calls

**Pros**:
- Can reduce repeated loads
- Simpler than server

**Cons**:
- Doesn't help with startup cost
- Complex cache invalidation
- Still pays import cost

**Decision**: Complementary, not sufficient alone

### Alternative 4: Per-Application Daemon

**Approach**: Each application runs its own persistent context

**Pros**:
- Application-specific optimization
- Simpler lifecycle

**Cons**:
- Doesn't generalize
- Multiple processes
- Duplicate effort

**Decision**: Rejected - prefer general solution

## Risks and Mitigations

### Risk 1: Complexity

**Risk**: Server mode adds significant complexity

**Mitigation**:
- Keep Phase 1 simple (proof of concept)
- Make server mode optional (not required)
- Comprehensive testing
- Clear documentation

### Risk 2: State Leakage

**Risk**: Requests accidentally share state, causing bugs

**Mitigation**:
- Stateless server design
- Request isolation enforced
- Comprehensive tests for isolation
- Clear documentation of constraints

### Risk 3: Process Management

**Risk**: Server lifecycle is fragile (crashes, hangs, leaks)

**Mitigation**:
- Automatic crash recovery
- Idle timeout
- Health checks
- Monitoring and logging

### Risk 4: Limited Improvement

**Risk**: Actual speedup is less than expected

**Mitigation**:
- Profile first (Phase 1)
- Measure baseline
- Set decision criteria
- Early validation before full implementation

### Risk 5: HTTP Overhead Worse Than Expected

**Risk**: HTTP overhead is higher than estimated (>5ms)

**Mitigation**:
- Measure actual overhead in Phase 1
- Even 10ms overhead gives 19x improvement (not 20x)
- Can optimize if needed (keep-alive, connection pooling)
- HTTP overhead is negligible vs 1,900ms startup cost

## Testing Strategy

### Unit Tests

- [ ] Server lifecycle (start, stop, restart)
- [ ] Request/response parsing
- [ ] Error handling
- [ ] Timeout behavior
- [ ] Data serialization/deserialization

### Integration Tests

- [ ] Client-server communication
- [ ] Multiple sequential requests
- [ ] Concurrent requests (future)
- [ ] Server crash recovery
- [ ] Long-running stability

### Performance Tests

- [ ] Startup time measurement
- [ ] Per-operation timing
- [ ] Memory usage tracking
- [ ] Benchmark vs direct mode

### Compatibility Tests

- [ ] Works with existing code (transparent)
- [ ] Falls back correctly when server unavailable
- [ ] lamd integration testing
- [ ] referia compatibility

## Success Metrics

**Phase 1 Decision Criteria**:
- [ ] Proof of concept works
- [ ] Performance improvement > 5x for 10 operations
- [ ] Implementation complexity acceptable

**Phase 2 Success Criteria**:
- [ ] lamd can use server mode in production
- [ ] All common operations work
- [ ] No known data corruption bugs
- [ ] Performance targets met

**Overall Success**:
- [ ] 10x speedup for 100 operations
- [ ] Adoption by lamd, other applications
- [ ] Positive user feedback
- [ ] Stable in production use

## Related

- Requirement: [REQ-0007](../requirements/req0007_fast-repeated-access.md) - Fast Repeated Access
  - **All acceptance criteria addressed** (see verification below)
- Backlog: [2026-01-03_profile-lynguine-startup-performance](../backlog/infrastructure/2026-01-03_profile-lynguine-startup-performance.md) (Completed)
- Tenets:
  - `explicit-infrastructure`: Server mode must maintain explicit behavior
  - `flow-based-processing`: Must work within flow-based processing model
- Related CIPs:
  - CIP-0007: Package separation (affects deployment model)

## Requirement Acceptance Criteria Verification

This CIP addresses all acceptance criteria from REQ-0007:

| Criterion | Status | How CIP Addresses |
|-----------|--------|-------------------|
| Repeated operations significantly faster | âœ… | 10-20x improvement (profiled and validated) |
| Startup overhead amortized | âœ… | Server mode design: 1 startup for N operations |
| Compatible with existing API | âœ… | Backward compatible, transparent client |
| Works for programmatic + CLI | âœ… | Both supported, same API |
| Data isolation (no state leakage) | âœ… | Stateless server design, isolated requests |
| Clear error messages | âœ… | Structured error responses, comprehensive handling |
| Memory reasonable for long-running | âœ… | 132 MB measured, stable footprint |
| Graceful shutdown and cleanup | âœ… | Idle timeout, health checks, request draining |
| Performance metrics documented | âœ… | Full profiling: 1.947s startup, 2ms per request |
| Cross-platform (Unix, Windows) | âœ… | HTTP/REST works on all platforms |

## Requirement Constraints Verification

This CIP respects all constraints from REQ-0007:

| Constraint | Status | How CIP Respects |
|------------|--------|------------------|
| Maintain explicitness (no hidden state) | âœ… | Stateless server, explicit requests/responses |
| Ensure data isolation | âœ… | Each request isolated, no shared state |
| Work within flow-based processing | âœ… | Server executes normal flow processing |
| Not break existing applications | âœ… | Optional server mode, graceful fallback |
| Work on all platforms | âœ… | HTTP/REST (not Unix sockets) |
| Handle crashes gracefully | âœ… | Client auto-recovery, server restart |
| Reasonable memory footprint | âœ… | 132 MB, same as normal lynguine |

## References

- Python http.server: https://docs.python.org/3/library/http.server.html
- Python requests library: https://docs.python-requests.org/
- REST API design: https://restfulapi.net/
- Python multiprocessing: https://docs.python.org/3/library/multiprocessing.html

## Author and Date

- **Author**: Neil Lawrence
- **Created**: 2026-01-03
- **Status**: Proposed (awaiting investigation and feedback)

## Next Steps

1. **Profile current performance** (establish baseline)
2. **Analyze lamd usage patterns** (understand requirements)
3. **Review this CIP** with team
4. **Decide**: Accept, revise, or explore alternatives
5. **If accepted**: Begin Phase 1 implementation

