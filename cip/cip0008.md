---
id: "cip0008"
title: "Lynguine Server Mode for Fast Repeated Access"
status: "Proposed"
priority: "High"
effort: "Large"
type: "architecture"
created: "2026-01-03"
last_updated: "2026-01-03"
owner: "lawrennd"
github_issue: null
dependencies: null
---

# CIP-0008: Lynguine Server Mode for Fast Repeated Access

## Status

- [x] Proposed: 2026-01-03
- [ ] Accepted
- [ ] Implemented
- [ ] Closed

## Description

Implement a server mode for lynguine that allows long-running processes to serve multiple data access requests without repeatedly paying startup costs. This addresses the performance requirement (REQ-0007) where applications like lamd need to access lynguine functionality multiple times in quick succession.

## Motivation

### The Problem

Applications like lamd currently call lynguine multiple times per operation:
- Each call requires full Python interpreter startup
- Import overhead (pandas, numpy, etc.) on every call
- Configuration parsing and initialization on every call
- Total time = (startup_cost × N_calls) + actual_work

**Current Performance** (needs measurement):
- Startup time: ? seconds (to be profiled)
- Per-operation overhead: ? seconds
- Total for N operations: startup × N

This makes lynguine impractical for:
- CLI tools that process batches of files
- Applications with frequent data access patterns
- Interactive workflows requiring multiple queries

### Why This Matters

**For lamd**:
- Needs to access multiple lynguine-managed files per operation
- Startup overhead dominates execution time
- Poor user experience

**For future applications**:
- Limits adoption for CLI tools
- Makes batch processing impractical
- Prevents interactive use cases

## Investigation and Analysis

### Profiling Results (Completed 2026-01-03)

**Environment**: macOS, Python 3.11 (conda), lynguine in development mode

**Key Finding**: Import costs dominate startup time at **~1.9 seconds per cold start**.

#### Startup Time Breakdown

```
Major Dependency Import Times:
  pandas              :  1.223s  (63% of total)
  numpy               :  0.000s  (already imported by pandas)
  yaml                :  0.019s  (1%)
  liquid              :  0.154s  (8%)

Lynguine Component Import Times:
  lynguine            :  0.548s  (28% of total)
  (submodules cached after initial import)

Summary:
  Total startup time:   1.947s
  Dependencies:         1.396s (72%)
  Lynguine components:  0.552s (28%)
```

**Analysis**:
- **pandas dominates**: 1.223s (63%) - cannot be optimized, must be amortized
- **lynguine initialization**: 0.548s (28%) - includes pandas triggering
- **Subsequent imports fast**: Cached after first import (Python module cache)

#### Memory Usage

```
Python baseline:           15.2 MB
After lynguine import:    132.4 MB  (+117 MB, mostly pandas/numpy)
After Interface import:   132.4 MB  (no additional overhead)
```

**Analysis**: Memory overhead dominated by pandas/numpy. Total footprint (~132 MB) is reasonable for long-running server process.

#### Performance Impact

**Current (without server mode)**:
- 1 call:    ~2.0s  (1.9s startup + 0.1s work)
- 10 calls:  ~20s   (19s startup + 1s work) - **95% overhead**
- 100 calls: ~200s  (190s startup + 10s work) - **95% overhead**

**Projected (with server mode)**:
- 1 call:    ~2.0s  (1x - no benefit)
- 10 calls:  ~3.0s  (**6-7x faster**)
- 100 calls: ~12s   (**15-20x faster**)

### Conclusion: Server Mode is Justified

**Evidence**:
1. ✅ Startup cost is high (1.9s per call)
2. ✅ Cannot be significantly optimized (pandas is core dependency)
3. ✅ Server mode would provide major benefit (10-20x for repeated calls)
4. ✅ Memory overhead acceptable (132 MB)
5. ✅ Real use case (lamd makes multiple calls per operation)

**Decision**: Proceed with CIP-0008 implementation.

## Proposed Solution: Lynguine Server Mode

### Architecture Overview

```
┌─────────────────────────────────────────────────────────────┐
│                    Client Applications                       │
│              (lamd, CLI tools, scripts)                      │
└────────────────────────┬────────────────────────────────────┘
                         │
                         ▼ (lightweight IPC)
┌─────────────────────────────────────────────────────────────┐
│                 Lynguine Server Process                      │
│                                                              │
│  ┌────────────────────────────────────────────────────┐    │
│  │  Server Manager                                     │    │
│  │  - Process lifecycle                                │    │
│  │  - Request routing                                  │    │
│  │  - Session management                               │    │
│  └────────────────────────────────────────────────────┘    │
│                         │                                    │
│                         ▼                                    │
│  ┌────────────────────────────────────────────────────┐    │
│  │  Lynguine Core (already imported, initialized)     │    │
│  │  - Interface, Context                               │    │
│  │  - CustomDataFrame                                  │    │
│  │  - Compute engine                                   │    │
│  └────────────────────────────────────────────────────┘    │
│                                                              │
└─────────────────────────────────────────────────────────────┘
```

### Key Design Decisions

#### 1. Server Lifecycle

**Question**: How is the server started and stopped?

**Proposal**:
- **Auto-start**: Client checks if server running, starts if needed
- **Graceful shutdown**: Timeout after idle period (configurable)
- **Manual control**: Commands to start/stop explicitly

```python
# Client usage (transparent to user)
from lynguine.client import Interface

# Auto-starts server if needed
interface = Interface.from_file("config.yml")
data = interface.read()
# Uses server for all operations
```

#### 2. Communication Protocol

**Options Evaluated**:
- **Unix domain sockets**: Fast, simple, Unix-only
- **HTTP/REST**: Universal, more overhead, well-understood
- **gRPC**: Efficient, typed, requires protobuf
- **Python multiprocessing**: Built-in, Python-specific

**Recommendation**: Start with Unix domain sockets (Phase 1), add HTTP later (Phase 2)

**Rationale**:
- Unix sockets: Lowest overhead, simplest implementation
- HTTP: Can add later for remote access, cross-platform
- Focus on getting basics working first

#### 3. Data Isolation

**Critical**: Each request must be isolated (no state leakage)

**Implementation**:
- Server maintains no request-level state
- Each operation is stateless
- Configuration passed with each request (or cached with session ID)
- Results serialized and sent back (no shared memory)

#### 4. Error Handling

**Challenges**:
- Server crash should not affect client
- Bad request should not crash server
- Clear error propagation to client

**Implementation**:
- Try/except around all request handlers
- Structured error responses
- Client detects server crash and restarts
- Logging on both client and server sides

### API Design

#### Client API (Backward Compatible)

```python
# Option 1: Transparent (preferred)
from lynguine import Interface
# Automatically uses server if available, falls back to direct

# Option 2: Explicit
from lynguine.client import ServerInterface
interface = ServerInterface.from_file("config.yml")

# Option 3: Context manager
from lynguine.server import LynguineServer
with LynguineServer() as server:
    # Multiple operations
    data1 = server.read("file1.yml")
    data2 = server.read("file2.yml")
```

#### Server Commands

```bash
# Start server manually
lynguine serve --daemon

# Stop server
lynguine serve --stop

# Status
lynguine serve --status

# Configure
lynguine serve --idle-timeout=300  # 5 minutes
```

### Wire Protocol

**Simple request/response over Unix socket**:

```python
# Request format (JSON)
{
    "id": "request-uuid",
    "method": "read_data",
    "params": {
        "config": {...},
        "path": "...",
        "options": {...}
    }
}

# Response format (JSON)
{
    "id": "request-uuid",
    "status": "success",  # or "error"
    "result": {...},      # serialized data
    "error": null         # or error message
}
```

**Data serialization**:
- JSON for metadata
- Pickle for DataFrames (fast, Python-native)
- Alternative: Arrow IPC format (language-agnostic)

## Implementation Plan

### Phase 1: Proof of Concept (2 weeks)

**Goal**: Validate approach with minimal implementation

- [x] Profile current startup costs (baseline measurements) - **COMPLETED 2026-01-03**
  - Startup: 1.947s (pandas 1.223s, lynguine 0.548s)
  - Memory: 132 MB
  - Target improvement validated: 10-20x possible
- [ ] Implement basic server (Unix sockets, single-threaded)
- [ ] Implement basic client (transparent connection)
- [ ] Support one operation: `read_data()`
- [ ] Benchmark performance vs direct calls
- [ ] Decision point: Is improvement sufficient?

**Success criteria**:
- Server starts and handles requests
- Client can make repeated calls
- Performance improvement > 5x for 10 operations (target: 6-7x based on profiling)

### Phase 2: Core Features (3-4 weeks)

**Goal**: Production-ready single-user server

- [ ] Support all common operations (read, write, compute)
- [ ] Graceful error handling
- [ ] Auto-start/stop lifecycle
- [ ] Idle timeout
- [ ] Logging and diagnostics
- [ ] Basic tests (unit and integration)
- [ ] Documentation (basic usage)

**Success criteria**:
- lamd can use server mode
- All operations work correctly
- No data corruption or state leakage
- Performance improvement validated

### Phase 3: Robustness (2-3 weeks)

**Goal**: Reliable, production-quality

- [ ] Comprehensive error handling
- [ ] Server crash recovery
- [ ] Request timeout handling
- [ ] Memory leak prevention
- [ ] Comprehensive test suite
- [ ] Performance benchmarks
- [ ] User documentation

**Success criteria**:
- 100+ tests passing
- No memory leaks in long runs
- Clear error messages
- Migration guide for users

### Phase 4: Advanced Features (future)

**Optional enhancements** (separate CIPs):

- [ ] HTTP/REST API for remote access
- [ ] Multi-user support (authentication)
- [ ] Request caching/memoization
- [ ] Parallel request handling
- [ ] Monitoring and metrics
- [ ] Cross-platform support (Windows named pipes)

## Performance Targets

**Based on profiling** (measured 2026-01-03):

| Scenario | Current | Target | Improvement |
|----------|---------|--------|-------------|
| Single call | ~2.0s | ~2.0s | 1x (no change) |
| 10 calls | ~20s | ~3.0s | **6-7x faster** |
| 100 calls | ~200s | ~12s | **15-20x faster** |

**Actual measurements**:
- ✅ Startup time: 1.947s (measured)
- ✅ Breakdown: pandas 1.223s, lynguine 0.548s, other 0.176s
- ✅ Memory usage: 132 MB (measured)
- ⏳ Server overhead per request: ~0.01s (estimated, to be validated in PoC)

## Backward Compatibility

**Critical**: Must not break existing code

**Strategy**:
- Server mode is **optional** (not required)
- Direct mode continues to work
- Client API is transparent (auto-detects server)
- Users can opt-in with `use_server=True` or environment variable

**Migration path**:
```python
# Old code (continues to work)
from lynguine import Interface
interface = Interface.from_file("config.yml")

# New code (same API, uses server if available)
from lynguine import Interface
interface = Interface.from_file("config.yml", use_server=True)

# Or via environment variable
export LYNGUINE_USE_SERVER=1
```

## Alternative Approaches Considered

### Alternative 1: Import Optimization Only

**Approach**: Defer heavy imports, optimize existing code

**Pros**:
- No architectural changes
- Simpler implementation
- No process management

**Cons**:
- Limited improvement (maybe 2-3x)
- Doesn't help with configuration parsing
- Still pays cost on every call

**Decision**: Worth doing as quick win, but insufficient alone

### Alternative 2: Persistent REPL

**Approach**: Keep Python process running, execute commands in context

**Pros**:
- Simpler than full server
- No serialization overhead

**Cons**:
- State management complexity
- Hard to isolate requests
- Error recovery difficult

**Decision**: Rejected - too fragile

### Alternative 3: In-Memory Caching

**Approach**: Cache loaded data in memory across calls

**Pros**:
- Can reduce repeated loads
- Simpler than server

**Cons**:
- Doesn't help with startup cost
- Complex cache invalidation
- Still pays import cost

**Decision**: Complementary, not sufficient alone

### Alternative 4: Per-Application Daemon

**Approach**: Each application runs its own persistent context

**Pros**:
- Application-specific optimization
- Simpler lifecycle

**Cons**:
- Doesn't generalize
- Multiple processes
- Duplicate effort

**Decision**: Rejected - prefer general solution

## Risks and Mitigations

### Risk 1: Complexity

**Risk**: Server mode adds significant complexity

**Mitigation**:
- Keep Phase 1 simple (proof of concept)
- Make server mode optional (not required)
- Comprehensive testing
- Clear documentation

### Risk 2: State Leakage

**Risk**: Requests accidentally share state, causing bugs

**Mitigation**:
- Stateless server design
- Request isolation enforced
- Comprehensive tests for isolation
- Clear documentation of constraints

### Risk 3: Process Management

**Risk**: Server lifecycle is fragile (crashes, hangs, leaks)

**Mitigation**:
- Automatic crash recovery
- Idle timeout
- Health checks
- Monitoring and logging

### Risk 4: Limited Improvement

**Risk**: Actual speedup is less than expected

**Mitigation**:
- Profile first (Phase 1)
- Measure baseline
- Set decision criteria
- Early validation before full implementation

### Risk 5: Platform Compatibility

**Risk**: Unix sockets don't work on Windows

**Mitigation**:
- Phase 1: Unix/Linux/macOS only
- Phase 4: Add Windows named pipes
- Fallback to direct mode on unsupported platforms

## Testing Strategy

### Unit Tests

- [ ] Server lifecycle (start, stop, restart)
- [ ] Request/response parsing
- [ ] Error handling
- [ ] Timeout behavior
- [ ] Data serialization/deserialization

### Integration Tests

- [ ] Client-server communication
- [ ] Multiple sequential requests
- [ ] Concurrent requests (future)
- [ ] Server crash recovery
- [ ] Long-running stability

### Performance Tests

- [ ] Startup time measurement
- [ ] Per-operation timing
- [ ] Memory usage tracking
- [ ] Benchmark vs direct mode

### Compatibility Tests

- [ ] Works with existing code (transparent)
- [ ] Falls back correctly when server unavailable
- [ ] lamd integration testing
- [ ] referia compatibility

## Success Metrics

**Phase 1 Decision Criteria**:
- [ ] Proof of concept works
- [ ] Performance improvement > 5x for 10 operations
- [ ] Implementation complexity acceptable

**Phase 2 Success Criteria**:
- [ ] lamd can use server mode in production
- [ ] All common operations work
- [ ] No known data corruption bugs
- [ ] Performance targets met

**Overall Success**:
- [ ] 10x speedup for 100 operations
- [ ] Adoption by lamd, other applications
- [ ] Positive user feedback
- [ ] Stable in production use

## Related

- Requirement: [REQ-0007](../requirements/req0007_fast-repeated-access.md)
- Tenets:
  - `explicit-infrastructure`: Server mode must maintain explicit behavior
  - `flow-based-processing`: Must work within flow-based processing model
- Related CIPs:
  - CIP-0007: Package separation (affects deployment model)

## References

- Unix domain sockets: https://docs.python.org/3/library/socket.html#socket.AF_UNIX
- Python multiprocessing: https://docs.python.org/3/library/multiprocessing.html
- gRPC Python: https://grpc.io/docs/languages/python/

## Author and Date

- **Author**: Neil Lawrence
- **Created**: 2026-01-03
- **Status**: Proposed (awaiting investigation and feedback)

## Next Steps

1. **Profile current performance** (establish baseline)
2. **Analyze lamd usage patterns** (understand requirements)
3. **Review this CIP** with team
4. **Decide**: Accept, revise, or explore alternatives
5. **If accepted**: Begin Phase 1 implementation

